{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 自动求梯度\n",
    "在深度学习中，我们经常需要对函数求梯度（gradient）。本节将介绍如何使用tensorflow2.0提供的GradientTape来自动求梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 简单示例\n",
    "---\n",
    "\n",
    "我们先看一个简单例子：对函数 $$y=2x^⊤x$$ 求关于列向量 x 的梯度。我们先创建变量x，并赋初值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reshape(tensor=tf.Variable(range(4),\\\n",
    "                                  dtype=tf.float32),shape=(4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数$$y=2x^⊤x$$ 关于x 的梯度应为$$4x$$ 。现在我们来验证一下求出来的梯度是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = 2 * tf.matmul(a=x,b=x,transpose_a=True)\n",
    "    \n",
    "dy_dx = t.gradient(target=y,sources=x)\n",
    "    # tf.GradientTape() \n",
    "        # doc: Record operations for automatic differentiatio\n",
    "        # .watch()\n",
    "            # doc: Ensures that `tensor` is being traced by this tape\n",
    "        # .gradient()\n",
    "            # doc: Computes the gradient using operations recorded in context of this tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[ 0.],\n",
       "       [ 4.],\n",
       "       [ 8.],\n",
       "       [12.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2 训练模式和预测模式\n",
    "---\n",
    "tf.GradientTape()参数\n",
    "\n",
    "- persistent: 布尔值，用来指定新创建的gradient tape是否是可持续性的。默认是False，意味着只能够调用一次gradient（）函数。\n",
    "- watch_accessed_variables: 布尔值，表明这个gradien tap是不是会自动追踪任何能被训练（trainable）的变量。默认是True。要是为False的话，意味着你需要手动去指定你想追踪的那些变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[  0.],\n",
       "        [  4.],\n",
       "        [ 32.],\n",
       "        [108.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[0.],\n",
       "        [2.],\n",
       "        [4.],\n",
       "        [6.]], dtype=float32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "    dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
    "    dy_dx = g.gradient(y, x)  # 6.0 (2*x at x =3)\n",
    "dz_dx,dy_dx\n",
    "\n",
    "    # 若presistent=False，则将会在第二次使用g.gradient()时报错：\n",
    "        #RuntimeError: GradientTape.gradient can only be called once on non-persistent tapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3 对Python控制流求梯度\n",
    "---\n",
    "即使函数的计算图包含了Python的控制流（如条件和循环控制），我们也有可能对变量求梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑下面程序，其中包含Python的条件和循环控制。需要强调的是，这里循环（while循环）迭代的次数和条件判断（if语句）的执行都取决于输入a的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "            # Computes the sum of elements across dimensions of a tensor.\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来分析一下上面定义的f函数。事实上，给定任意输入a，其输出必然是 f(a) = x * a的形式，其中标量系数x的值取决于输入a。由于c = f(a)有关a的梯度为x，且值为c / a，我们可以像下面这样验证对本例中控制流求梯度的结果的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[ True]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal((1,1),dtype=tf.float32)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(a)\n",
    "    c = f(a)\n",
    "t.gradient(c,a) == c/a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
